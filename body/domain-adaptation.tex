% !TEX root =  ../main.tex
Structured prediction problems such as SRL require substantial amounts of training data. 
SRL systems such as MATE typically train on resources such as FrameNet, which contain more than 190,000 sentences. 
Semantic roles are not reliably identified with syntactic patterns alone. 
A pattern such as {\em [verb] to [X]} could suggest that X is a result or enabler depending on the process at hand. 
For instance, {\em change to [X]} indicates a resulting state, whereas {\em adapts to [X]} doesn't. 
This suggests that lexical information (e.g., the type of verb) is quite critical. 
Unless the lexical variations had all been observed in the training data, generalization is likely to suffer. 
We explore two ideas to address this issue.

\subsection{Domain Adaptation}
\label{sec:domain-adaptation}
A straightforward approach to training the SRL system is to combine all process sentences into one pool and learn a single model. 
As an alternate approach, we can learn a SRL model for every process using only the sentences that describe the process. 
This enables learning from a much smaller but highly relevant set of sentences. 
Note this is possible in our setting because we know beforehand which process each sentence is describing.
Rather than picking one approach over the other, we can combine their strengths using domain adaptation ideas~\cite{daume2009frustratingly}.

The sentences that describe the target process can be viewed as target domain data, and the sentences describing all other processes can be viewed as the source domain data. Following~\cite{daume2009frustratingly} we utilize a simple approach for domain adaptation. The key idea is to take the existing features and create a new feature vector that contains three versions of the original features. A source-specific version, a target-specific version, and a general version. The instances from the source domain will only contain the general and source-specific versions, and the instances for the target domain will contain the general and the target-specific versions. Formally, if $\mathbf{f}$ is the set of features used, then we use the following mappings to create new feature vectors:\\
\begin{align*}
\Phi^{s}(f) &= <f, f, 0>\\
\Phi^{t}(f) &= <f, 0, f>\\
\end{align*}

This mapping enables the learning algorithm to do domain adaptation by learning two sets of weights that reflect the utility of a feature across all domains as well as within the target domain. This simple transformation has been shown to be quite effective for domain adaptation~\cite{daume2009frustratingly}.




