% !TEX root =  ../main.tex
\section{Evaluation}

\subsection{Process Recognition Collection}

\subsubsection{Questions}

Our data set contains 141 process identification questions, which were selected manually out of 4650 questions from Help Teaching\footnote{http://www.helpteaching.com}, a collection of tests and worksheets for parents and educators, and 195 questions from the 4th-grade New York Regents Science Exams collection, similar to the one used in (Clark et al., 2013). We selected multiple-choice questions where at least two of the answer choices, including the correct answer, were processes.

\subsubsection{Process Sentences}

For each question, we identified all processes given as answer choices and collected definition sentences for each. In total, we collected 948 definition sentences covering 183 processes. These sentences were obtained from a variety of sources such as Barron's Study Guides and various web resources including Wikipedia and WordNet. \todo{Expand on sources for definitions?} 

\subsubsection{Semantic Role Annotations}

Each question and definition sentence was annotated to according to the following set of guidelines to identify the roles expressed by the sentence. A sentence could contain multiple (or no) values for a single role, but text spans are not allowed to overlap - that is, the same word or phrase cannot be used for multiple roles.

After the original annotations were completed, they were checked for quality by a second annotator.

\subsection{Semantic Role Labeling}
\begin{table*}[htdp]
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Method			& 	Precision	&	Recall	&	F1\\
\hline
Standard			& 	0.4323	& 	0.3325	&	0.3758\\
Per Process		&	0.4225	& 	0.2556	& 	0.3185\\
Distant Supervision 	& 	{\bf 0.5614}	& 	0.2642	& 	0.3594 \\
Dom. Adaptation	& 	0.4386	& 	{\bf 0.3351}	& 	{\bf 0.3799}\\
\hline
\end{tabular}
\end{center}
\caption{Semantic Role Labeling Performance. Bold face entries indicate the best performance.}
\label{tab:srl-results}
\end{table*}%

First, we compare the performance of SRL using MATE under the following configurations:
\begin{itemize}
\item {\em Standard} -- This configuration uses sentences from all processes combined together for training and does not distinguish at test time whether the sentence is describing a particular process.
\item {\em Per-Process} -- This uses only the target process sentences for training. This setting requires that we have some seed set of sentences for every process.
\item {\em Domain Adaptation} -- This uses both target process sentences as well other sentences using the domain adaptation technique described in Section~\ref{sec:domain-adaptation}.
\item {\em Distant Supervision} -- This setting uses the sentences obtained via distant supervision for training. 
However, it trains only on the sentences that describe the target process and does not use sentences describing the other processes. 
This requires that we have some seed knowledge for each process but not necessarily sentence-level annotations.
\end{itemize}

We use 5-fold cross validation to test each configuration.
Table~\ref{tab:srl-results} shows the results. 
The results show that MATE is not highly effective with the best F1 of around 0.38. 
This is substantially lower compared to the state-of-the-art performance of MATE on standard datasets such as CoNLL, where the performance around 0.80 in F1. 
We believe that the limited amount of training data is a key factor in the performance differences.
Training on target domain sentences using the Per-Process model results in lower performance, whereas Standard, Distant Supervision, and Domain Adaptation, all with increased amounts of data result in better performance. 

We find that Domain Adaptation provides minor improvements over the Standard model. Interestingly, Distant Supervision doesn't improve F1 but achieves the best overall precision. 
We hypothesize that this is in part due to the change in distribution of the roles. 
Many Distant Supervision annotated sentences do not contain all the roles. 
Thus, the prior probability of observing any particular role decreases potentially causing the learner to be more conservative about predicting roles.  
\begin{table}[htdp]
\begin{center}
\begin{tabular}{|l|c|}
\hline
Method & Accuracy\\
\hline
BOW & 63.12\\
Manual SRL & 67.38\\
BOW+Manual SRL & {\bf 70.92}\\
\hline
Standard	& {\bf 55.32}\\
Per Process & 46.80 \\
Domain Adaptation & {\bf 55.32}\\
Distant Supervision & 51.77\\
\hline
BOW + Standard  & {\bf 65.24}\\
\hline
\end{tabular}
\caption{Question Answering Performance. Bold face indicates the best performance in each block.}
\label{tab:qa-results}
\end{center}
\vspace{-3ex}
\end{table}

\subsection{QA}
Our central hypothesis is that using semantic-role based representation helps in process recognition questions. 
We compare manually generated semantic roles (as a result of the annotation process), and the performance of automatically derived semantic roles for question answering. 
We use the process described in Section~\ref{sec:qa} for answering questions. 

Table~\ref{tab:qa-results} shows the accuracy of the various systems. 
As a baseline, we use a simple bag-of-words system that uses textual entailment but without the semantic roles (BOW). 
Manual SRL, is a system which uses manually assigned semantic roles as its representation. 
We find that using manual roles by itself for alignment yields a 9\% relative improvement in accuracy. 
BOW and SRL perform well in slightly different subsets of questions. 
As a result combining SRL-based scoring with BOW based scoring yields more than 12\% improvement. 
While access to semantic roles provides gains, a variety of other issues also need to be addressed to further improve QA performance (Section~\ref{sec:error-analysis}).

Automatically obtained semantic roles using the Standard formulation is worse than BOW by itself. 
However, in conjunction with BOW, Standard SRL provides minor gains over using BOW alone. 
For the most part, the QA performance of the other automatic SRL variants track the SRL extraction accuracy. 
This suggests that improving the automatic SRL performance is likely to provide improvements in QA. 

\input{body/error-analysis}